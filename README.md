# ğŸš€ ArxivBench
*â€œFresh from ArXiv, served once, and never reheated.â€*

> ğŸ“Œ TL;DR: ArxivBench tells you **â€œHow much of your score is real, and how much is cheating?â€**  

---

## 1. ğŸ“Š What is ArxivBench?

ArxivBench is a **dynamic, one-time-pad-inspired evaluation framework** ğŸ›¡ï¸ that **audits** how much Large Language Models (LLMs) **over-estimate** their true abilities on public benchmarks.  

### âš ï¸ Key Problems ArxivBench Tackles  
- **ğŸ“¥ Data contamination**  
  Public benchmarks (MMLU, GSM8K, etc.) often sneak into pre-training data â†’ inflated scores.  
- **ğŸ¯ Biased overtraining**  
  Developers may â€œteach to the test,â€ tuning models only on popular domains.  
- **ğŸ•µï¸ Transparency crisis**  
  Private leaderboards (SEAL, Chatbot Arena) are opaque & hard to reproduce.

---

### ğŸ§ª How ArxivBench Works  

1. **ğŸŒ± Fresh Test Cases**  
   Every 6 months we scrape **latest ArXiv preprints** (Aprâ€“Sep 2024 â†’ ArxivBench-2024b).  
   > ğŸ·ï¸ Domains: CS, Math, Physics, Bio, Econ, Finance, Statistics, EE.

2. **ğŸ² SCP Tasks**  
   Articles are auto-converted into three symbolic tasks:  
   - **Sequencing** ğŸ”€ â†’ Re-order shuffled sentences.  
   - **Cloze** ğŸ•³ï¸ â†’ Fill masked sentences.  
   - **Prediction** ğŸ”® â†’ Choose the correct next sentence.  

3. **ğŸ“ˆ Rugged Scores (RS)**  
   - **RS-I** ğŸ§ª = % inflation on public vs. private benchmarks.  
   - **RS-II** âš–ï¸ = performance variance across domains (biased training detector).

---

### ğŸŒŸ Unique Features  
- **ğŸ• One-Time Use**: private benchmarks are **used once**, then **expired & open-sourced**.  
- **âœ… High Quality**: filtered for length, complexity, minimal math/tables.  
- **ğŸŒ Broad Coverage**: 8 domains, ~100-word contexts, 1 k+ samples per domain.

---

## ğŸ‘©â€ğŸ’» 2. How Do I Evaluate my Model?

The most easy way is to use `llm-eval-harness`

Just install `lm-eval` from [here](https://github.com/liangzid/harness-4-arxivbench),
and then evaluate a huggingface model with:

```sh

export task_ls=(
    "arxivbench2024b_all_setcsSCP-s" \
    "arxivbench2024b_all_setcsSCP-c" \
    "arxivbench2024b_all_setcsSCP-p" \
    "arxivbench2024b_all_setq-finSCP-s" \
    "arxivbench2024b_all_setq-finSCP-c" \
    "arxivbench2024b_all_setq-finSCP-p" \
    "arxivbench2024b_all_setmathSCP-s" \
    "arxivbench2024b_all_setmathSCP-c" \
    "arxivbench2024b_all_setmathSCP-p" \
    "arxivbench2024b_all_seteessSCP-s" \
    "arxivbench2024b_all_seteessSCP-c" \
    "arxivbench2024b_all_seteessSCP-p" \
    "arxivbench2024b_all_setphysicsSCP-s" \
    "arxivbench2024b_all_setphysicsSCP-c" \
    "arxivbench2024b_all_setphysicsSCP-p" \
    "arxivbench2024b_all_setstatSCP-s" \
    "arxivbench2024b_all_setstatSCP-c" \
    "arxivbench2024b_all_setstatSCP-p" \
    "arxivbench2024b_all_setq-bioSCP-s" \
    "arxivbench2024b_all_setq-bioSCP-c" \
    "arxivbench2024b_all_setq-bioSCP-p" \
    "arxivbench2024b_all_seteconSCP-s" \ 
    "arxivbench2024b_all_seteconSCP-c" \
    "arxivbench2024b_all_seteconSCP-p" 
)
	lm_eval\
	    --model hf\
	    --model_args pretrained=your-model-name,parallelize=True\
	    --tasks any-arxivbench-task\
	    --verbosity DEBUG\
	    --log_samples\
	    --output_path your-log-path
```

You can also evaluate LLM via APIs with examples detailed in `./eval/`.




## ğŸ‘©â€ğŸ’» 3. How to Use & Read the Code

---

### ğŸ“¦ 1. Install Environment

#### Via pip
```bash
pip install -r re.txt
```

#### Via conda (recommended)
```bash
conda env create -f robench.yaml
conda activate robench
```

#### Clone & editable install
```bash
git clone https://github.com/XXXXXX/harness-4-robench/tree/robench
cd harness-4-robench
pip install -e .
```

---

### ğŸ—‚ï¸ 2. File Map â€“ â€œWhere is what?â€

| ğŸ“ Path | ğŸ¯ Purpose |
|---------|------------|
| `./1.run_vanilla_construct.py` | ğŸ—ï¸ **One-click generator** of your **private benchmark** from fresh ArXiv papers. |
| `constructor.py` | ğŸ”§ **Engine room**: all SCP logic (Sequencing / Cloze / Prediction) lives here. |
| `data/` | ğŸ“š **Static assets** |
| &nbsp;&nbsp;&nbsp;&nbsp;`./data/INSTRUCTION.py` | ğŸ“ Prompt templates fed into the LLM during evaluation. |
| `eval/` | ğŸ§ª **Evaluation scripts** |
| &nbsp;&nbsp;&nbsp;&nbsp;`./eval/0.1.vanilla_harness_test.sh` | ğŸ¤— Evaluate **open-source HuggingFace models**. |
| &nbsp;&nbsp;&nbsp;&nbsp;`./eval/0.2.harness_eval_closeAIs.sh` | ğŸ” Evaluate **OpenAI / Claude / Gemini APIs**. |
| `post_process_paper_text.py` | âœ‚ï¸ Clean & segment raw ArXiv LaTeX â†’ plain English sentences. |
| `spider_arxiv.py` | ğŸ•·ï¸ Crawler that **downloads** the latest ArXiv PDFs & abstracts. |
| `SearchBySomething.py` | ğŸ” TF-IDF retriever to mine distractor sentences for Prediction tasks. |
| `Vectorize.py` | ğŸ§® Convert any text into dense embeddings for retrieval. |
| `utils.py` | ğŸ§° Tiny helpers (date parsing, logging, helpers, etc.). |

---

### ğŸš€ 3. Quick Start â€“ 3 Commands to Glory

#### â‘  Download ArXiv articles
```bash
python spider_arxiv.py
```
> ğŸ—‚ï¸ Drops papers into `./data/raw/`.

#### â‘¡ Build a private benchmark
```bash
python 1.run_vanilla_construct.py
```
> ğŸ² Generates **Sequencing / Cloze / Prediction** tasks â†’ `./benchmarks/`.

#### â‘¢ Reproduce our Results
```bash
# Open-source models (Llama, Qwen, ...)
bash ./eval/0.1.vanilla_harness_test.sh

# Proprietary APIs (GPT-4, Claude, Gemini, ...)
bash ./eval/0.2.harness_eval_closeAIs.sh
```
> ğŸ“Š Results saved as JSON + auto-plot RS scores.

---

Happy benchmarking! ğŸª„













